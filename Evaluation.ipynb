{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad15f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b780819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test answer data set\n",
    "with open('true_answers.json', 'r') as f:\n",
    "    true_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd133ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        predictions = json.load(file)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c88af476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data set\n",
    "def parse_years(years_str: str) -> List[int]:\n",
    "    \n",
    "    years_str = years_str.replace('–', '-').replace('—', '-').replace('−', '-')\n",
    "    # 处理 \"since\" 关键字\n",
    "    years_str = re.sub(r'since (\\d{4})', r'\\1-', years_str)\n",
    "    years = []\n",
    "    for part in years_str.split(', '):\n",
    "        if '-' in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split('-'))\n",
    "                years.extend(range(start, end + 1))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "                #print(f\"Skipping invalid year range part: {part}. Error: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                years.append(int(part))\n",
    "            except ValueError as e:\n",
    "                #print(f\"Skipping invalid year part: {part}. Error: {e}\")\n",
    "                continue\n",
    "    return years\n",
    "\n",
    "def parse_answer(answer: str) -> Tuple[List[str], Dict[str, List[int]]]:\n",
    "    entities = []\n",
    "    timelines = {}\n",
    "    parts = re.split(r'\\),\\s*', answer)\n",
    "    \n",
    "    parts = [part + ')' if not part.endswith(')') else part for part in parts]\n",
    "\n",
    "    for part in parts:\n",
    "        entity_match = re.match(r'(.+?)\\s+\\((.+)\\)', part)\n",
    "        if entity_match:\n",
    "            entity = entity_match.group(1).strip()\n",
    "            years_str = entity_match.group(2).strip()\n",
    "            years = parse_years(years_str)\n",
    "            entities.append(entity)\n",
    "            timelines[entity] = years\n",
    "        else:\n",
    "            continue\n",
    "            #print(f\"Skipping invalid part: {part}\")\n",
    "    return entities, timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e2d7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM of entities\n",
    "def evaluate_entities_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_questions = 0\n",
    "    matching_questions = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        if set(pred_entities) == set(gt_entities) and set(pred_entities):\n",
    "            matching_questions += 1\n",
    "            print(f\"Matched entities: {set(pred_entities)}\")\n",
    "        total_questions += 1\n",
    "\n",
    "    entity_em_score = matching_questions / total_questions if total_questions > 0 else 0\n",
    "    return entity_em_score\n",
    "\n",
    "\n",
    "# EM of timeline \n",
    "def evaluate_timeline_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_entities = 0\n",
    "    matching_timelines = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        if set(pred_entities) == set(gt_entities) and set(pred_entities):\n",
    "            for entity in gt_entities:\n",
    "                if entity in pred_entities and entity in pred_timelines and entity in gt_timelines:\n",
    "                    if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                        matching_timelines += 1\n",
    "                    total_entities += 1\n",
    "\n",
    "    timeline_em_score = matching_timelines / total_entities if total_entities > 0 else 0\n",
    "    return timeline_em_score\n",
    "\n",
    "\n",
    "def evaluate_completeness(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_completeness = 0\n",
    "    num_questions = len(parsed_true_answers)\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        \n",
    "        correct_entities = set(pred_entities).intersection(set(gt_entities))\n",
    "        correct_timelines = 0\n",
    "        total_gt_timelines = len(gt_entities)  \n",
    "\n",
    "        for entity in correct_entities:\n",
    "            if entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    correct_timelines += 1\n",
    "\n",
    "        completeness = correct_timelines / total_gt_timelines if total_gt_timelines > 0 else 0\n",
    "        total_completeness += completeness\n",
    "\n",
    "    average_completeness = total_completeness / num_questions if num_questions > 0 else 0\n",
    "    return average_completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e711c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score \n",
    "\n",
    "def evaluate_entity_f1(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> Tuple[float, float, float]:\n",
    "    total_true_positives = 0\n",
    "    total_predicted = 0\n",
    "    total_actual = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        true_positives = len(set(pred_entities).intersection(set(gt_entities)))\n",
    "        total_true_positives += true_positives\n",
    "        total_predicted += len(pred_entities)\n",
    "        total_actual += len(gt_entities)\n",
    "\n",
    "    return precision_recall_f1(total_true_positives, total_predicted, total_actual)\n",
    "\n",
    "def evaluate_timeline_f1(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> Tuple[float, float, float]:\n",
    "    total_true_positives = 0\n",
    "    total_predicted = 0\n",
    "    total_actual = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        for entity in gt_entities:\n",
    "            if entity in pred_entities and entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    total_true_positives += 1\n",
    "                total_actual += 1\n",
    "\n",
    "        total_predicted += len(pred_timelines)\n",
    "\n",
    "    return precision_recall_f1(total_true_positives, total_predicted, total_actual)\n",
    "\n",
    "def precision_recall_f1(true_positives, predicted, actual):\n",
    "    precision = true_positives / predicted if predicted > 0 else 0\n",
    "    recall = true_positives / actual if actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18f6ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(predictions: List[str], true_answers: List[List[str]]) -> Tuple[float, float, float]:\n",
    "    parsed_predictions = [parse_answer(pred) for pred in predictions]\n",
    "    parsed_true_answers = [\n",
    "        (\n",
    "            list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "            {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "        ) \n",
    "        for ans in true_answers\n",
    "    ]\n",
    "\n",
    "    \n",
    "    \n",
    "    _, _, entity_f1_score = evaluate_entity_f1(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    _, _, timeline_f1_score = evaluate_timeline_f1(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    \n",
    "    entities_em_score = evaluate_entities_em(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    timeline_em_score =  evaluate_timeline_em(parsed_predictions, parsed_true_answers)\n",
    "\n",
    "\n",
    "    completeness_score = evaluate_completeness(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    return entity_f1_score, timeline_f1_score, entities_em_score, timeline_em_score, completeness_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "212bd532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_true_answers = [\n",
    "    (\n",
    "        list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "        {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "    ) \n",
    "    for ans in true_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5d48262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched entities: {'Jason Kenney', 'Jim Prentice', 'Rachel Notley'}\n",
      "Matched entities: {'Republican Party', 'Democratic Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "File: predictions_3shots_flanxl.json\n",
      "Entity EM Score: 0.003734827264239029\n",
      "Timeline EM Score:0.0\n",
      "Entity F1 Score: 0.04353586977096347\n",
      "Timeline F1 Score: 0.020537897310513448\n",
      "Completeness: 0.007847583477835578\n",
      "\n",
      "Matched entities: {'Republican Party', 'Democratic Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "Matched entities: {'Arseniy Yatsenyuk', 'Volodymyr Groysman', 'Oleksiy Honcharuk', 'Serhiy Arbuzov', 'Denys Shmyhal', 'Yulia Tymoshenko', 'Mykola Azarov'}\n",
      "File: predictions_5shots_flanxl.json\n",
      "Entity EM Score: 0.003734827264239029\n",
      "Timeline EM Score:0.5384615384615384\n",
      "Entity F1 Score: 0.04824726724462872\n",
      "Timeline F1 Score: 0.027477919528949953\n",
      "Completeness: 0.009803921568627449\n",
      "\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'The People of Freedom', 'Forza Italia'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "Matched entities: {'Arseniy Yatsenyuk', 'Volodymyr Groysman', 'Oleksiy Honcharuk', 'Serhiy Arbuzov', 'Denys Shmyhal', 'Yulia Tymoshenko', 'Mykola Azarov'}\n",
      "Matched entities: {'Republican Party', 'Democratic Party'}\n",
      "File: predictions_7shots_flanxl.json\n",
      "Entity EM Score: 0.004668534080298786\n",
      "Timeline EM Score:0.6\n",
      "Entity F1 Score: 0.04408132243967319\n",
      "Timeline F1 Score: 0.026694045174537988\n",
      "Completeness: 0.009492685963274199\n",
      "\n",
      "File: predictions_10shots_flanxl.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Entity F1 Score: 0.009411764705882352\n",
      "Timeline F1 Score: 0.00425531914893617\n",
      "Completeness: 0.0012449424214130095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# closed book\n",
    "# withouot fine-tuning\n",
    "# Evaluation of flan xl\n",
    "\n",
    "files_xl = [\n",
    "    \"predictions_3shots_flanxl.json\",\n",
    "    \"predictions_5shots_flanxl.json\",\n",
    "    \"predictions_7shots_flanxl.json\",\n",
    "    \"predictions_10shots_flanxl.json\"\n",
    "]\n",
    "\n",
    "for file in files_xl:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    \n",
    "    entity_f1, timeline_f1, entity_em, timeline_em, completeness = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bdb5dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: predictions_3shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Entity F1 Score: 0.022461538461538463\n",
      "Timeline F1 Score: 0.008281573498964804\n",
      "Completeness: 0.005239132690113081\n",
      "\n",
      "File: predictions_5shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Entity F1 Score: 0.026003824091778205\n",
      "Timeline F1 Score: 0.018590998043052837\n",
      "Completeness: 0.008092125739184563\n",
      "\n",
      "File: predictions_7shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Entity F1 Score: 0.01935099732063114\n",
      "Timeline F1 Score: 0.00613325899080011\n",
      "Completeness: 0.004668534080298785\n",
      "\n",
      "Matched entities: {'Republican Party', 'Libertarian Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "File: predictions_10shot_flanlarge.json\n",
      "Entity EM Score: 0.0018674136321195146\n",
      "Timeline EM Score:0.0\n",
      "Entity F1 Score: 0.010509296685529508\n",
      "Timeline F1 Score: 0.004563605248146035\n",
      "Completeness: 0.0017117958294428882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of flan large\n",
    "\n",
    "files_large = [\n",
    "    \"predictions_3shot_flanlarge.json\",\n",
    "    \"predictions_5shot_flanlarge.json\",\n",
    "    \"predictions_7shot_flanlarge.json\",\n",
    "    \"predictions_10shot_flanlarge.json\"\n",
    "]\n",
    "\n",
    "for file in files_large:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    \n",
    "    entity_f1, timeline_f1, entity_em, timeline_em, completeness = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
