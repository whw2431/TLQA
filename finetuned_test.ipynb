{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvPknhdun5Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3626f833-2ccb-4604-a832-b64e94a2b53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "\u001b[31mERROR: Invalid requirement: 'huggingface_hub!'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install nltk\n",
        "# !pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install tokenizers\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install sentencepiece\n",
        "!pip install huggingface_hub!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
        "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "PpYtOnppGhjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2UVg9rxG6W3",
        "outputId": "7f7bbf1d-98c5-475d-b810-c026c0f0d1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/finetuned_flant5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr_ZjfzaHhQ-",
        "outputId": "ac287904-571b-443f-b3a2-59cd6a353354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/finetuned_flant5.zip\n",
            "   creating: content/results/checkpoint-1000/\n",
            "  inflating: content/results/checkpoint-1000/config.json  \n",
            "  inflating: content/results/checkpoint-1000/model.safetensors  \n",
            "  inflating: content/results/checkpoint-1000/added_tokens.json  \n",
            "  inflating: content/results/checkpoint-1000/trainer_state.json  \n",
            "  inflating: content/results/checkpoint-1000/optimizer.pt  \n",
            "  inflating: content/results/checkpoint-1000/spiece.model  \n",
            "  inflating: content/results/checkpoint-1000/training_args.bin  \n",
            "  inflating: content/results/checkpoint-1000/special_tokens_map.json  \n",
            "  inflating: content/results/checkpoint-1000/tokenizer_config.json  \n",
            "  inflating: content/results/checkpoint-1000/scheduler.pt  \n",
            "  inflating: content/results/checkpoint-1000/generation_config.json  \n",
            "  inflating: content/results/checkpoint-1000/rng_state.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_checkpoint = \"/content/content/results/checkpoint-1000\"\n",
        "\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
        "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPtAr7riKC2q",
        "outputId": "69125fe8-b6cd-4966-c611-68426d5ff081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = \"List all positions Oleksandr Turchynov, also known as Oleksandr Valentynovych Turchynov, held from 2010 to 2020.\"\n",
        "inputs = \"Please answer to this question: \" + my_question\n",
        "\n",
        "inputs = tokenizer.encode(inputs,return_tensors=\"pt\")\n",
        "outputs = finetuned_model.generate(\n",
        "    inputs,\n",
        "    max_length=100,  # Increase this value if outputs are incomplete\n",
        "    temperature=0.0000001,\n",
        "    do_sample=True, # Adjust temperature for diversity\n",
        "    # top_k=50,        # Adjust top-k sampling\n",
        "    # top_p=0.95,      # Adjust top-p sampling\n",
        "    repetition_penalty=2.0  # Apply repetition penalty\n",
        ")\n",
        "answer = tokenizer.decode(outputs[0])\n",
        "from textwrap import fill\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iRExMS-LFaE",
        "outputId": "1c80bbc0-b0af-4845-b1bf-964d4580e099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> Minister of Foreign Affairs (2010, 2011, 2012, 2013, 2014) President of the Russian Federation (2014, 2015, 2016, 2017, 2018, 2019, 2020)</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh9DfDT8gRQA",
        "outputId": "8ebd86bb-8471-457e-e897-15ec7a15fb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "def read_QA_data(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return data\n",
        "\n",
        "class KnnSearch:\n",
        "    def __init__(self,data=None, num_trees=None,emb_dim=None):\n",
        "        self.num_trees=num_trees\n",
        "        self.emb_dim=emb_dim\n",
        "    def get_embeddings_for_data(self, data_ls):\n",
        "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        embeddings = model.encode(data_ls)\n",
        "        return embeddings\n",
        "\n",
        "    def get_top_n_neighbours(self,sentence,data_emb,train_data,k):\n",
        "        sent_emb = self.get_embeddings_for_data(sentence)\n",
        "        #data_emb = self.get_embeddings_for_data(transfer_questions)\n",
        "        top_questions = []\n",
        "\n",
        "        print(\"new_emb\", sent_emb.shape, data_emb.shape)\n",
        "        text_sims = cosine_similarity(data_emb,[sent_emb]).tolist()\n",
        "        results_sims = zip(range(len(text_sims)), text_sims)\n",
        "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        #print(\"text_sims\",sorted_similarities[:2])\n",
        "        for idx, item in sorted_similarities[:k]:\n",
        "                #if item[0] > 0.45:\n",
        "                    top_questions.append(train_data[idx])\n",
        "\n",
        "        return top_questions\n",
        "\n",
        "def get_questions(train_data):\n",
        "    train_questions = []\n",
        "    for index, data in enumerate(train_data):\n",
        "        train_questions.append(data[\"question\"])\n",
        "    return train_questions\n",
        "\n",
        "def get_prompts(exemplars):\n",
        "    prompt = \"\"\n",
        "    for index, data in enumerate(exemplars):\n",
        "        # prompt+=\"[Question {}]: \".format(index+1)+ data[\"question\"]+\"\\n\"\n",
        "        prompt+=\"Example {}: \".format(index+1)+\"\\n\"\n",
        "        prompt+= data[\"question\"]+\"\\n\"\n",
        "        answer = \"\"\n",
        "        for ind, subans in enumerate(data[\"answers\"]):\n",
        "          answer+=subans\n",
        "          if(ind+1 < len(data[\"answers\"])):\n",
        "            answer+=', '\n",
        "        # prompt+=\"[Answer {}]: \".format(index+1)+ answer+\"\\n\"\n",
        "        prompt+= answer+\"\\n\"\n",
        "\n",
        "    print(\"prompt§§§§\",prompt)\n",
        "    return prompt\n",
        "\n",
        "train_data = read_QA_data(\"train_TLQA.json\")\n",
        "test_data = read_QA_data(\"test_TLQA.json\")"
      ],
      "metadata": {
        "id": "6eBTKQYbgIqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_instance = KnnSearch()\n",
        "\n",
        "train_questions = get_questions(train_data)\n",
        "data_emb = knn_instance.get_embeddings_for_data(train_questions)\n",
        "\n",
        "def few_shot_input(query, data_emb, train_data, k):\n",
        "  exemplars = knn_instance.get_top_n_neighbours(query,data_emb,train_data,k)\n",
        "  prompt_question = get_prompts(exemplars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8EtH3MNgXTz",
        "outputId": "df463b51-eaa5-4a59-cc5b-d8aa4780e677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.9 --quiet"
      ],
      "metadata": {
        "id": "cxBoQM7phN3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def process_answer(answers):\n",
        "  final_answer = \"\"\n",
        "  for answer in answers:\n",
        "    final_answer+= (answer+' ')\n",
        "\n",
        "  return final_answer\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Lower text and remove punctuation, articles and extra whitespace.\n",
        "\n",
        "    Args:\n",
        "        s: String to normalize.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned string with lowercase, no punctuations, no articles, and\n",
        "            and extraneous whitespace.\n",
        "    \"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Calculates F1 score.\n",
        "\n",
        "    Args:\n",
        "        prediction: Predicted answer span (string).\n",
        "        ground_truth: True answer span (string).\n",
        "\n",
        "    Returns:\n",
        "        F1 score.\n",
        "    \"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "\n",
        "    ground_truth = process_answer(ground_truth)\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"Calculates exact match (EM) score.\n",
        "\n",
        "    Args:\n",
        "        prediction: Predicted answer span (string).\n",
        "        ground_truth: True answer span (string).\n",
        "\n",
        "    Returns:\n",
        "        EM score.\n",
        "    \"\"\"\n",
        "    ground_truth = process_answer(ground_truth)\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n"
      ],
      "metadata": {
        "id": "_nTfNfjMgg3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "gen_cfg = GenerationConfig.from_pretrained(last_checkpoint)"
      ],
      "metadata": {
        "id": "fL9kMsgUhuG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe = pipeline(task=\"text2text-generation\", model=finetuned_model, tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "EKZK9REMhXnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.utils import filter_complex_metadata\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "def db_generate(text):\n",
        "  # web_loader = JSONLoader(\n",
        "  #     file_path=filename,\n",
        "  #     jq_schema='.info',\n",
        "  #     text_content=False)\n",
        "\n",
        "  # web_doc = web_loader.load()\n",
        "  # # print(web_doc)\n",
        "  # updated_web_doc = filter_complex_metadata(web_doc)\n",
        "\n",
        "\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "  # chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "  # print(len(chunked_web_doc))\n",
        "  chunked_web_doc = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "  db_web = FAISS.from_documents(chunked_web_doc, embeddings)\n",
        "  return db_web\n"
      ],
      "metadata": {
        "id": "Okb1UeEDhl1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu==1.7.2 --quiet"
      ],
      "metadata": {
        "id": "heAQbgtRjHO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from tqdm import tqdm\n",
        "from textwrap import fill\n",
        "import json\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Ensure that your answer follows the same format as provided in the examples.\n",
        "prompt_template = \"\"\"\n",
        "Please answer the question based on the context provided. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "Context:\n",
        "{context}{instruction}\n",
        "Question: {question}\n",
        "\"\"\".format(context = \"{context}\", instruction = \"\", question = \"{question}\")\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "EM_total = 0\n",
        "F1_total = 0\n",
        "with open('RAG.json') as f:\n",
        "    d = json.load(f)\n",
        "    RAG_data = d['info']\n",
        "\n",
        "\n",
        "for i in tqdm(range(0,10)):\n",
        "  query = test_data[i]['question']\n",
        "  answers = test_data[i]['answers']\n",
        "  # wikiid = test_data[i]['wikidata_ID']\n",
        "  # crawler_summary(wikiid)\n",
        "  db_web = db_generate(RAG_data[i])\n",
        "\n",
        "  Chain_web = RetrievalQA.from_chain_type(\n",
        "      llm=llm,\n",
        "      chain_type=\"stuff\",\n",
        "      # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "      # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "      # k defines how many documents are returned; defaults to 4.\n",
        "      # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "      # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "      retriever=db_web.as_retriever(search_kwargs={'k': 3}), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "      chain_type_kwargs={\"prompt\": prompt},\n",
        "  )\n",
        "\n",
        "  result = Chain_web.invoke(query)\n",
        "  print(result)\n",
        "  EM_total+=exact_match_score(result['result'], answers)\n",
        "  F1_total+=f1_score(result['result'], answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgndOWBngsBl",
        "outputId": "d3cb880a-a030-487d-b091-e92ea63b2999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 10%|█         | 1/10 [00:35<05:23, 35.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all sports teams Anthony Grant, also known as Anthony Paul Shaun Andrew Daure Grant, played for from 2010 to 2020.', 'result': 'Chelsea F.C. (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018) Shrewsbury Town F.C. (2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 20%|██        | 2/10 [01:09<04:38, 34.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all positions Oleksandr Turchynov, also known as Oleksandr Valentynovych Turchynov, held from 2010 to 2020.', 'result': 'President of Ukraine (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020) President of Ukraine (2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 30%|███       | 3/10 [01:24<02:59, 25.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all political parties Paul Murphy was a member of from 2010 to 2020.', 'result': 'Socialist Party (2010, 2011, 2012, 2013, 2014) Teachta Dála (2014, 2015, 2016, 2017, 2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 40%|████      | 4/10 [01:52<02:38, 26.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all sports teams Andy Carroll, also known as Andrew Thomas Carroll, played for from 2010 to 2020.', 'result': 'West Ham United F.C. (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019) Liverpool F.C. (2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 50%|█████     | 5/10 [03:24<04:11, 50.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all heads of the government of Newfoundland and Labrador, also known as NL, from 2013 to 2020 ', 'result': 'Joan Marie Aylward (2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020) John McLean (2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 60%|██████    | 6/10 [03:35<02:27, 36.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all employers Samy Azer, also known as Samy A Azer, worked for from 2010 to 2020.', 'result': 'University of Sydney (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020) University of Melbourne (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 70%|███████   | 7/10 [04:15<01:53, 37.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all positions Leo Varadkar, also known as Leo Eric Varadkar, held from 2010 to 2020.', 'result': 'Minister for Social Services (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019) Minister for Trade and Industry (2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 80%|████████  | 8/10 [04:22<00:55, 27.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all coaches of Belarus national under-21 football team from 2010 to 2020', 'result': 'Andrei Zhukov (2010, 2011, 2012, 2013) Sergei Kostyan (2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            " 90%|█████████ | 9/10 [05:01<00:31, 31.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all sports teams Joe Ledley, also known as Joseph Christopher Ledley, played for from 2010 to 2020.', 'result': 'Cardiff City F.C. (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017) England national under-17 football team (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020) England national under-21 football team (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020) England national under-21 football team (2010) England national association football team (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 10/10 [05:16<00:00, 31.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'List all political parties Laurențiu Rebega was a member of from 2010 to 2020.', 'result': 'Romanian Conservative Party (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018) independent politician (2018, 2019, 2020)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Exact Match: ', EM_total/len(test_data))\n",
        "print('F1 Score: ', F1_total/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWaPdu8Tj0aG",
        "outputId": "96427c3a-8824-4a11-fe85-6a852c795cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match:  0.0\n",
            "F1 Score:  0.5969065412653884\n"
          ]
        }
      ]
    }
  ]
}