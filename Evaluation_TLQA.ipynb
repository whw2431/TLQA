{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139ecd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee2e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test answer data set\n",
    "with open('true_answers.json', 'r') as f:\n",
    "    true_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65bdcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        predictions = json.load(file)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16890f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data set\n",
    "def parse_years(years_str: str) -> List[int]:\n",
    "    \n",
    "    years_str = years_str.replace('–', '-').replace('—', '-').replace('−', '-')\n",
    "    years = []\n",
    "    for part in years_str.split(', '):\n",
    "        if '-' in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split('-'))\n",
    "                years.extend(range(start, end + 1))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "                #print(f\"Skipping invalid year range part: {part}. Error: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                years.append(int(part))\n",
    "            except ValueError as e:\n",
    "                #print(f\"Skipping invalid year part: {part}. Error: {e}\")\n",
    "                continue\n",
    "    return years\n",
    "\n",
    "def parse_answer(answer: str) -> Tuple[List[str], Dict[str, List[int]]]:\n",
    "    entities = []\n",
    "    timelines = {}\n",
    "    parts = re.split(r'\\),\\s*', answer)\n",
    "    \n",
    "    parts = [part + ')' if not part.endswith(')') else part for part in parts]\n",
    "\n",
    "    for part in parts:\n",
    "        entity_match = re.match(r'(.+?)\\s+\\((.+)\\)', part)\n",
    "        if entity_match:\n",
    "            entity = entity_match.group(1).strip()\n",
    "            years_str = entity_match.group(2).strip()\n",
    "            years = parse_years(years_str)\n",
    "            entities.append(entity)\n",
    "            timelines[entity] = years\n",
    "        else:\n",
    "            continue\n",
    "            #print(f\"Skipping invalid part: {part}\")\n",
    "    return entities, timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b58cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM of entities\n",
    "def evaluate_entities_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_entities = 0\n",
    "    matching_entities = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        total_entities += len(gt_entities)\n",
    "        matching_entities += len(set(pred_entities).intersection(set(gt_entities)))\n",
    "\n",
    "    entity_em_score = matching_entities / total_entities if total_entities > 0 else 0\n",
    "    return entity_em_score\n",
    "\n",
    "# EM of timeline \n",
    "def evaluate_timeline_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_entities = 0\n",
    "    matching_timelines = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        for entity in gt_entities:\n",
    "            if entity in pred_entities and entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    matching_timelines += 1\n",
    "                total_entities += 1\n",
    "\n",
    "    timeline_em_score = matching_timelines / total_entities if total_entities > 0 else 0\n",
    "    return timeline_em_score\n",
    "\n",
    "\n",
    "# Completness\n",
    "def evaluate_completeness(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_completeness = 0\n",
    "    num_questions = len(parsed_true_answers)\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        correct_entities = set(pred_entities).intersection(set(gt_entities))\n",
    "        completeness = len(correct_entities) / len(gt_entities) if len(gt_entities) > 0 else 0\n",
    "        total_completeness += completeness\n",
    "\n",
    "    average_completeness = total_completeness / num_questions if num_questions > 0 else 0\n",
    "    return average_completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6590e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score \n",
    "\n",
    "def calculate_entity_precision_recall(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> Tuple[float, float, int]:\n",
    "    true_positives = 0\n",
    "    predicted_entities = 0\n",
    "    actual_entities = 0\n",
    "    correct_entities_set = []\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        correct_entities = set(pred_entities).intersection(set(gt_entities))\n",
    "        true_positives += len(correct_entities)\n",
    "        predicted_entities += len(pred_entities)\n",
    "        actual_entities += len(gt_entities)\n",
    "\n",
    "        correct_entities_set.append(correct_entities)\n",
    "\n",
    "    precision = true_positives / predicted_entities if predicted_entities > 0 else 0\n",
    "    recall = true_positives / actual_entities if actual_entities > 0 else 0\n",
    "    return precision, recall, correct_entities_set\n",
    "\n",
    "def calculate_timeline_precision_recall(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]], correct_entities_set: List[set]) -> Tuple[float, float]:\n",
    "    true_positives = 0\n",
    "    predicted_timelines = 0\n",
    "    actual_timelines = 0\n",
    "\n",
    "    for pred, gt, correct_entities in zip(parsed_predictions, parsed_true_answers, correct_entities_set):\n",
    "        _, pred_timelines = pred\n",
    "        _, gt_timelines = gt\n",
    "\n",
    "        for entity in correct_entities:\n",
    "            if entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    true_positives += 1\n",
    "                predicted_timelines += 1\n",
    "                actual_timelines += 1\n",
    "\n",
    "    precision = true_positives / predicted_timelines if predicted_timelines > 0 else 0\n",
    "    recall = true_positives / actual_timelines if actual_timelines > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_f1_score(precision: float, recall: float) -> float:\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33cc1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(predictions: List[str], true_answers: List[List[str]]) -> Tuple[float, float, float]:\n",
    "    parsed_predictions = [parse_answer(pred) for pred in predictions]\n",
    "    parsed_true_answers = [\n",
    "        (\n",
    "            list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "            {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "        ) \n",
    "        for ans in true_answers\n",
    "    ]\n",
    "\n",
    "    entity_precision, entity_recall, correct_entities_set = calculate_entity_precision_recall(parsed_predictions, parsed_true_answers)\n",
    "    entity_f1_score = calculate_f1_score(entity_precision, entity_recall)\n",
    "\n",
    "    timeline_precision, timeline_recall = calculate_timeline_precision_recall(parsed_predictions, parsed_true_answers, correct_entities_set)\n",
    "    timeline_f1_score = calculate_f1_score(timeline_precision, timeline_recall)\n",
    "\n",
    "    completeness_score = evaluate_completeness(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    return entity_f1_score, timeline_f1_score, completeness_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4997d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_true_answers = [\n",
    "    (\n",
    "        list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "        {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "    ) \n",
    "    for ans in true_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "884813d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: predictions_3shots_flanxl.json\n",
      "Entity EM Score: 0.036346396965865994\n",
      "Timeline EM Score:0.1810344827586207\n",
      "Entity F1 Score: 0.04353586977096347\n",
      "Timeline F1 Score: 0.1826086956521739\n",
      "Completeness: 0.03968470579114838\n",
      "\n",
      "File: predictions_5shots_flanxl.json\n",
      "Entity EM Score: 0.040455120101137804\n",
      "Timeline EM Score:0.21875\n",
      "Entity F1 Score: 0.04824726724462872\n",
      "Timeline F1 Score: 0.21875\n",
      "Completeness: 0.04379566641471405\n",
      "\n",
      "File: predictions_7shots_flanxl.json\n",
      "Entity EM Score: 0.03666245259165613\n",
      "Timeline EM Score:0.22413793103448276\n",
      "Entity F1 Score: 0.04408132243967319\n",
      "Timeline F1 Score: 0.22413793103448276\n",
      "Completeness: 0.04389231476066211\n",
      "\n",
      "File: predictions_10shots_flanxl.json\n",
      "Entity EM Score: 0.00695322376738306\n",
      "Timeline EM Score:0.13636363636363635\n",
      "Entity F1 Score: 0.009411764705882352\n",
      "Timeline F1 Score: 0.13636363636363635\n",
      "Completeness: 0.008564166407303661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# closed book\n",
    "# withouot fine-tuning\n",
    "# Evaluation of flan xl\n",
    "\n",
    "files_xl = [\n",
    "    \"predictions_3shots_flanxl.json\",\n",
    "    \"predictions_5shots_flanxl.json\",\n",
    "    \"predictions_7shots_flanxl.json\",\n",
    "    \"predictions_10shots_flanxl.json\"\n",
    "]\n",
    "\n",
    "for file in files_xl:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    parsed_predictions = [parse_answer(pred) for pred in predictions]\n",
    "    entity_em = evaluate_entities_em(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    timeline_em = evaluate_timeline_em(parsed_predictions, parsed_true_answers)\n",
    "\n",
    "    entity_f1, timeline_f1, completeness = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ff681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: predictions_3shot_flanlarge.json\n",
      "Entity EM Score: 0.02307206068268015\n",
      "Timeline EM Score:0.1917808219178082\n",
      "Entity F1 Score: 0.022461538461538463\n",
      "Timeline F1 Score: 0.1917808219178082\n",
      "Completeness: 0.023889185303751122\n",
      "\n",
      "File: predictions_5shot_flanlarge.json\n",
      "Entity EM Score: 0.021491782553729456\n",
      "Timeline EM Score:0.27941176470588236\n",
      "Entity F1 Score: 0.026003824091778205\n",
      "Timeline F1 Score: 0.27941176470588236\n",
      "Completeness: 0.02448090347250011\n",
      "\n",
      "File: predictions_7shot_flanlarge.json\n",
      "Entity EM Score: 0.02054361567635904\n",
      "Timeline EM Score:0.16923076923076924\n",
      "Entity F1 Score: 0.01935099732063114\n",
      "Timeline F1 Score: 0.16923076923076924\n",
      "Completeness: 0.024513509107346638\n",
      "\n",
      "File: predictions_10shot_flanlarge.json\n",
      "Entity EM Score: 0.008217446270543615\n",
      "Timeline EM Score:0.15384615384615385\n",
      "Entity F1 Score: 0.010509296685529508\n",
      "Timeline F1 Score: 0.15384615384615385\n",
      "Completeness: 0.00980132793858284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of flan large\n",
    "\n",
    "files_large = [\n",
    "    \"predictions_3shot_flanlarge.json\",\n",
    "    \"predictions_5shot_flanlarge.json\",\n",
    "    \"predictions_7shot_flanlarge.json\",\n",
    "    \"predictions_10shot_flanlarge.json\"\n",
    "]\n",
    "\n",
    "for file in files_large:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    parsed_predictions = [parse_answer(pred) for pred in predictions]\n",
    "    entity_em = evaluate_entities_em(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    timeline_em = evaluate_timeline_em(parsed_predictions, parsed_true_answers)\n",
    "\n",
    "    entity_f1, timeline_f1, completeness = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
