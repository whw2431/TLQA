{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c94cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55b8ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test answer data set\n",
    "with open('true_answers.json', 'r') as f:\n",
    "    true_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5e75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        predictions = json.load(file)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a153fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data set\n",
    "def parse_years(years_str: str) -> List[int]:\n",
    "    \n",
    "    years_str = years_str.replace('–', '-').replace('—', '-').replace('−', '-')\n",
    "    # 处理 \"since\" 关键字\n",
    "    years_str = re.sub(r'since (\\d{4})', r'\\1-', years_str)\n",
    "    years = []\n",
    "    for part in years_str.split(', '):\n",
    "        if '-' in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split('-'))\n",
    "                years.extend(range(start, end + 1))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "                #print(f\"Skipping invalid year range part: {part}. Error: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                years.append(int(part))\n",
    "            except ValueError as e:\n",
    "                #print(f\"Skipping invalid year part: {part}. Error: {e}\")\n",
    "                continue\n",
    "    return years\n",
    "\n",
    "def parse_answer(answer: str) -> Tuple[List[str], Dict[str, List[int]]]:\n",
    "    entities = []\n",
    "    timelines = {}\n",
    "    parts = re.split(r'\\),\\s*', answer)\n",
    "    \n",
    "    parts = [part + ')' if not part.endswith(')') else part for part in parts]\n",
    "\n",
    "    for part in parts:\n",
    "        entity_match = re.match(r'(.+?)\\s+\\((.+)\\)', part)\n",
    "        if entity_match:\n",
    "            entity = entity_match.group(1).strip()\n",
    "            years_str = entity_match.group(2).strip()\n",
    "            years = parse_years(years_str)\n",
    "            entities.append(entity)\n",
    "            timelines[entity] = years\n",
    "        else:\n",
    "            continue\n",
    "            #print(f\"Skipping invalid part: {part}\")\n",
    "    return entities, timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3725f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM of entities\n",
    "def evaluate_entities_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_questions = 0\n",
    "    matching_questions = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        if set(pred_entities) == set(gt_entities) and set(pred_entities):\n",
    "            matching_questions += 1\n",
    "            print(f\"Matched entities: {set(pred_entities)}\")\n",
    "        total_questions += 1\n",
    "\n",
    "    entity_em_score = matching_questions / total_questions if total_questions > 0 else 0\n",
    "    return entity_em_score\n",
    "\n",
    "\n",
    "# EM of timeline \n",
    "def evaluate_timeline_em(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_entities = 0\n",
    "    matching_timelines = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        if set(pred_entities) == set(gt_entities) and set(pred_entities):\n",
    "            for entity in gt_entities:\n",
    "                if entity in pred_entities and entity in pred_timelines and entity in gt_timelines:\n",
    "                    if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                        matching_timelines += 1\n",
    "                    total_entities += 1\n",
    "\n",
    "    timeline_em_score = matching_timelines / total_entities if total_entities > 0 else 0\n",
    "    return timeline_em_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "021d0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_completeness(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_completeness = 0\n",
    "    num_questions = len(parsed_true_answers)\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        \n",
    "        correct_entities = set(pred_entities).intersection(set(gt_entities))\n",
    "        correct_timelines = 0\n",
    "        total_gt_timelines = len(gt_entities)  \n",
    "\n",
    "        for entity in correct_entities:\n",
    "            if entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    correct_timelines += 1\n",
    "\n",
    "        completeness = correct_timelines / total_gt_timelines if total_gt_timelines > 0 else 0\n",
    "        total_completeness += completeness\n",
    "\n",
    "    average_completeness = total_completeness / num_questions if num_questions > 0 else 0\n",
    "    return average_completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e267c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entity_completeness(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> float:\n",
    "    total_completeness = 0\n",
    "    num_questions = len(parsed_true_answers)\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        correct_entities = set(pred_entities).intersection(set(gt_entities))\n",
    "        total_gt_entities = len(gt_entities)  \n",
    "\n",
    "        completeness = len(correct_entities) / total_gt_entities if total_gt_entities > 0 else 0\n",
    "        total_completeness += completeness\n",
    "\n",
    "    average_completeness = total_completeness / num_questions if num_questions > 0 else 0\n",
    "    return average_completeness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f96d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score \n",
    "\n",
    "def evaluate_entity_f1(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> Tuple[float, float, float]:\n",
    "    total_true_positives = 0\n",
    "    total_predicted = 0\n",
    "    total_actual = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, _ = pred\n",
    "        gt_entities, _ = gt\n",
    "\n",
    "        true_positives = len(set(pred_entities).intersection(set(gt_entities)))\n",
    "        total_true_positives += true_positives\n",
    "        total_predicted += len(pred_entities)\n",
    "        total_actual += len(gt_entities)\n",
    "\n",
    "    return precision_recall_f1(total_true_positives, total_predicted, total_actual)\n",
    "\n",
    "def evaluate_timeline_f1(parsed_predictions: List[Tuple[List[str], Dict[str, List[int]]]], parsed_true_answers: List[Tuple[List[str], Dict[str, List[int]]]]) -> Tuple[float, float, float]:\n",
    "    total_true_positives = 0\n",
    "    total_predicted = 0\n",
    "    total_actual = 0\n",
    "\n",
    "    for pred, gt in zip(parsed_predictions, parsed_true_answers):\n",
    "        pred_entities, pred_timelines = pred\n",
    "        gt_entities, gt_timelines = gt\n",
    "\n",
    "        for entity in gt_entities:\n",
    "            if entity in pred_entities and entity in pred_timelines and entity in gt_timelines:\n",
    "                if pred_timelines[entity] == gt_timelines[entity]:\n",
    "                    total_true_positives += 1\n",
    "                total_actual += 1\n",
    "\n",
    "        total_predicted += len(pred_timelines)\n",
    "\n",
    "    return precision_recall_f1(total_true_positives, total_predicted, total_actual)\n",
    "\n",
    "def precision_recall_f1(true_positives, predicted, actual):\n",
    "    precision = true_positives / predicted if predicted > 0 else 0\n",
    "    recall = true_positives / actual if actual > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def process_answer(answers):\n",
    "    \n",
    "    final_answer = \"\"\n",
    "    for answer in answers:\n",
    "        \n",
    "        final_answer+= (answer+' ')\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"\n",
    "    Lower text and remove punctuation, articles and extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        s: String to normalize.\n",
    "\n",
    "    Returns:\n",
    "        Cleaned string with lowercase, no punctuations, no articles, and\n",
    "            and extraneous whitespace.\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Calculates F1 score.\n",
    "\n",
    "    Args:\n",
    "        prediction: Predicted answer span (string).\n",
    "        ground_truth: True answer span (string).\n",
    "\n",
    "    Returns:\n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "\n",
    "    ground_truth = process_answer(ground_truth)\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1da28829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(predictions: List[str], true_answers: List[List[str]]) -> Tuple[float, float, float]:\n",
    "    parsed_predictions = [parse_answer(pred) for pred in predictions]\n",
    "    parsed_true_answers = [\n",
    "        (\n",
    "            list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "            {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "        ) \n",
    "        for ans in true_answers\n",
    "    ]\n",
    "\n",
    "    \n",
    "    overall_f1_score = f1_score(\" \".join(predictions), [\" \".join(ans) for ans in true_answers])\n",
    "    _, _, entity_f1_score = evaluate_entity_f1(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    _, _, timeline_f1_score = evaluate_timeline_f1(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    \n",
    "    entities_em_score = evaluate_entities_em(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    timeline_em_score =  evaluate_timeline_em(parsed_predictions, parsed_true_answers)\n",
    "\n",
    "\n",
    "    completeness_score = evaluate_completeness(parsed_predictions, parsed_true_answers)\n",
    "    entity_completeness_score = evaluate_entity_completeness(parsed_predictions, parsed_true_answers)\n",
    "    \n",
    "    return overall_f1_score, entity_f1_score, timeline_f1_score, entities_em_score, timeline_em_score, completeness_score, entity_completeness_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2855fab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_true_answers = [\n",
    "    (\n",
    "        list(map(lambda x: re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(), ans)), \n",
    "        {re.match(r'(.+?)\\s+\\((.+)\\)', x).group(1).strip(): parse_years(re.match(r'(.+?)\\s+\\((.+)\\)', x).group(2).strip()) for x in ans}\n",
    "    ) \n",
    "    for ans in true_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae82f9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched entities: {'Jim Prentice', 'Rachel Notley', 'Jason Kenney'}\n",
      "Matched entities: {'Democratic Party', 'Republican Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "File: predictions_3shots_flanxl.json\n",
      "Entity EM Score: 0.003734827264239029\n",
      "Timeline EM Score:0.0\n",
      "Overall F1 Score: 0.5571964956195243\n",
      "Entity F1 Score: 0.04353586977096347\n",
      "Timeline F1 Score: 0.020537897310513448\n",
      "Completeness: 0.007847583477835578\n",
      "Entity Completness: 0.03968470579114838\n",
      "\n",
      "Matched entities: {'Democratic Party', 'Republican Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "Matched entities: {'Serhiy Arbuzov', 'Yulia Tymoshenko', 'Arseniy Yatsenyuk', 'Oleksiy Honcharuk', 'Mykola Azarov', 'Denys Shmyhal', 'Volodymyr Groysman'}\n",
      "File: predictions_5shots_flanxl.json\n",
      "Entity EM Score: 0.003734827264239029\n",
      "Timeline EM Score:0.5384615384615384\n",
      "Overall F1 Score: 0.5836406671437222\n",
      "Entity F1 Score: 0.04824726724462872\n",
      "Timeline F1 Score: 0.027477919528949953\n",
      "Completeness: 0.009803921568627449\n",
      "Entity Completness: 0.04379566641471405\n",
      "\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "Matched entities: {'The People of Freedom', 'Forza Italia'}\n",
      "Matched entities: {'Chief of the Defence Staff', 'Chief of the General Staff'}\n",
      "Matched entities: {'Serhiy Arbuzov', 'Yulia Tymoshenko', 'Arseniy Yatsenyuk', 'Oleksiy Honcharuk', 'Mykola Azarov', 'Denys Shmyhal', 'Volodymyr Groysman'}\n",
      "Matched entities: {'Democratic Party', 'Republican Party'}\n",
      "File: predictions_7shots_flanxl.json\n",
      "Entity EM Score: 0.004668534080298786\n",
      "Timeline EM Score:0.6\n",
      "Overall F1 Score: 0.580661267058693\n",
      "Entity F1 Score: 0.04408132243967319\n",
      "Timeline F1 Score: 0.026694045174537988\n",
      "Completeness: 0.009492685963274199\n",
      "Entity Completness: 0.04389231476066211\n",
      "\n",
      "File: predictions_10shots_flanxl.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Overall F1 Score: 0.46665657635840774\n",
      "Entity F1 Score: 0.009411764705882352\n",
      "Timeline F1 Score: 0.00425531914893617\n",
      "Completeness: 0.0012449424214130095\n",
      "Entity Completness: 0.008564166407303661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# closed book\n",
    "# withouot fine-tuning\n",
    "# Evaluation of flan xl\n",
    "\n",
    "files_xl = [\n",
    "    \"predictions_3shots_flanxl.json\",\n",
    "    \"predictions_5shots_flanxl.json\",\n",
    "    \"predictions_7shots_flanxl.json\",\n",
    "    \"predictions_10shots_flanxl.json\"\n",
    "]\n",
    "\n",
    "for file in files_xl:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    \n",
    "    overall_f1, entity_f1, timeline_f1, entity_em, timeline_em, completeness,entity_com = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nOverall F1 Score: {overall_f1}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\nEntity Completness: {entity_com}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3d44621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: predictions_3shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Overall F1 Score: 0.5633279683050638\n",
      "Entity F1 Score: 0.022461538461538463\n",
      "Timeline F1 Score: 0.008281573498964804\n",
      "Completeness: 0.005239132690113081\n",
      "Entity Completness: 0.023889185303751122\n",
      "\n",
      "File: predictions_5shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Overall F1 Score: 0.5188428724786711\n",
      "Entity F1 Score: 0.026003824091778205\n",
      "Timeline F1 Score: 0.018590998043052837\n",
      "Completeness: 0.008092125739184563\n",
      "Entity Completness: 0.02448090347250011\n",
      "\n",
      "File: predictions_7shot_flanlarge.json\n",
      "Entity EM Score: 0.0\n",
      "Timeline EM Score:0\n",
      "Overall F1 Score: 0.57602983556535\n",
      "Entity F1 Score: 0.01935099732063114\n",
      "Timeline F1 Score: 0.00613325899080011\n",
      "Completeness: 0.004668534080298785\n",
      "Entity Completness: 0.024513509107346638\n",
      "\n",
      "Matched entities: {'Libertarian Party', 'Republican Party'}\n",
      "Matched entities: {'United States representative', 'United States senator'}\n",
      "File: predictions_10shot_flanlarge.json\n",
      "Entity EM Score: 0.0018674136321195146\n",
      "Timeline EM Score:0.0\n",
      "Overall F1 Score: 0.4748468750907138\n",
      "Entity F1 Score: 0.010509296685529508\n",
      "Timeline F1 Score: 0.004563605248146035\n",
      "Completeness: 0.0017117958294428882\n",
      "Entity Completness: 0.00980132793858284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of flan large\n",
    "\n",
    "files_large = [\n",
    "    \"predictions_3shot_flanlarge.json\",\n",
    "    \"predictions_5shot_flanlarge.json\",\n",
    "    \"predictions_7shot_flanlarge.json\",\n",
    "    \"predictions_10shot_flanlarge.json\"\n",
    "]\n",
    "\n",
    "for file in files_large:\n",
    "    \n",
    "    predictions = load_predictions(file)\n",
    "    \n",
    "    overall_f1, entity_f1, timeline_f1, entity_em, timeline_em, completeness,entity_com = calculate_scores(predictions, true_answers)\n",
    "    print(f\"File: {file}\\nEntity EM Score: {entity_em}\\nTimeline EM Score:{timeline_em}\\nOverall F1 Score: {overall_f1}\\nEntity F1 Score: {entity_f1}\\nTimeline F1 Score: {timeline_f1}\\nCompleteness: {completeness}\\nEntity Completness: {entity_com}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
